# ğŸ§  NLP Language Modeling â€“ Exercise 1

This repository contains the implementation of **Exercise 1** from the Natural Language Processing course at the Hebrew University.

---

## ğŸ“š Overview

The goal of this exercise is to build **Unigram** and **Bigram** language models based on a given corpus.  
We compute word probabilities, sentence perplexities, and complete sentences using statistical modeling.

We use **Linear Interpolation** to combine unigram and bigram models for more accurate predictions.

---

## ğŸ§ª Features

- Train unigram and bigram language models on large text corpora
- Calculate word and sentence probabilities
- Compute perplexity for given sentences
- Perform sentence completion using probabilistic models
- Use linear interpolation:  
  `P(wáµ¢|wáµ¢â‚‹â‚) = Î»â‚ Â· P_unigram(wáµ¢) + Î»â‚‚ Â· P_bigram(wáµ¢|wáµ¢â‚‹â‚)`

---

## ğŸ› ï¸ Technologies Used

- Python 3.x
- `spaCy` for tokenization and preprocessing
- `NumPy` for vectorized probability calculations
- WikiText dataset (as `.parquet`)

---

## ğŸ“ Files Included

| File                       | Description                                         |
|----------------------------|-----------------------------------------------------|
| `ex1.py`                  | Main script with implementation of the models       |
| `Ex1 - NLP.pdf`           | Assignment instructions (ignored by `.gitignore`)   |
| `Practical Part.pdf/docx` | Output and explanation (ignored by `.gitignore`)    |
| `.gitignore`              | Ignores large files like `.zip`, `.pdf`, `.parquet` |

---

## ğŸš€ How to Run

1. Install dependencies:
   ```bash
   pip install spacy numpy
   python -m spacy download en_core_web_sm
